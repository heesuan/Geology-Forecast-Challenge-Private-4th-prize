{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd07YNpbvUuV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from scipy.interpolate import PchipInterpolator, UnivariateSpline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# ------------------- Data Loading -------------------\n",
        "\n",
        "train = pd.read_csv('/kaggle/input/geology-forecast-challenge-open/data/train.csv')\n",
        "test = pd.read_csv('/kaggle/input/geology-forecast-challenge-open/data/test.csv')\n",
        "\n",
        "# Save geology_id for later use in submission\n",
        "test_ids = test[\"geology_id\"]\n",
        "\n",
        "# Split train into input (X: -299 to 0) and target (y: 1 to 300)\n",
        "X_train_full = train.iloc[:, 1:301]\n",
        "y_train_full = train.iloc[:, 301:601]\n",
        "\n",
        "# Extract input columns from test data\n",
        "X_test_full = test.iloc[:, 1:301]\n",
        "\n",
        "# ------------------- Missing Value Utilities -------------------\n",
        "\n",
        "# Find first observed column index (i.e., first non-NaN)\n",
        "def find_k(series):\n",
        "    notna_index = series.notna()\n",
        "    if notna_index.any():\n",
        "        return int(series.index[notna_index.argmax()])\n",
        "    return None  # Return None if all values are NaN\n",
        "\n",
        "# Linear interpolation\n",
        "def Linear_prediction(x_known, y_known, x_missing):\n",
        "    model = LinearRegression()\n",
        "    model.fit(x_known.reshape(-1, 1), y_known)\n",
        "    return model.predict(x_missing.reshape(-1, 1))\n",
        "\n",
        "# PCHIP interpolation\n",
        "def PCHIP_prediction(x_known, y_known, x_missing):\n",
        "    interpolator = PchipInterpolator(x_known, y_known, extrapolate=True)\n",
        "    return interpolator(x_missing)\n",
        "\n",
        "# KNN interpolation\n",
        "def KNN_prediction(x_known, y_known, x_missing, k=3):\n",
        "    model = KNeighborsRegressor(n_neighbors=min(k, len(x_known)))\n",
        "    model.fit(x_known.reshape(-1, 1), y_known)\n",
        "    return model.predict(x_missing.reshape(-1, 1))\n",
        "\n",
        "# Univariate spline interpolation\n",
        "def Spline_prediction(x_known, y_known, x_missing, s=0):\n",
        "    spline = UnivariateSpline(x_known, y_known, s=s, ext=0)\n",
        "    return spline(x_missing)\n",
        "\n",
        "# Evaluate smoothness using variance of second derivative\n",
        "def evaluate_smoothness(y_full):\n",
        "    dy = np.gradient(y_full)\n",
        "    ddy = np.gradient(dy)\n",
        "    return np.var(ddy)\n",
        "\n",
        "# Unified imputation function using multiple methods\n",
        "def predict_missing_values(data):\n",
        "    filled_data = data.copy()\n",
        "\n",
        "    for i in range(data.shape[0]):\n",
        "        row_data = data.iloc[i, 1:301]\n",
        "        k_value = find_k(row_data)\n",
        "        if k_value is None:\n",
        "            continue\n",
        "\n",
        "        k_index = row_data.index.get_loc(str(k_value))\n",
        "        x_known = row_data.iloc[k_index+1:].dropna()\n",
        "        y_target = row_data.iloc[:k_index]\n",
        "        if x_known.empty or y_target.empty:\n",
        "            continue\n",
        "\n",
        "        x_known_idx = x_known.index.astype(float).to_numpy()\n",
        "        y_known = x_known.to_numpy()\n",
        "        x_missing_idx = y_target.index.astype(float).to_numpy()\n",
        "\n",
        "        candidates = {}\n",
        "        try:\n",
        "            candidates[\"linear\"] = Linear_prediction(x_known_idx, y_known, x_missing_idx)\n",
        "        except:\n",
        "            pass\n",
        "        try:\n",
        "            candidates[\"pchip\"] = PCHIP_prediction(x_known_idx, y_known, x_missing_idx)\n",
        "        except:\n",
        "            pass\n",
        "        try:\n",
        "            candidates[\"knn\"] = KNN_prediction(x_known_idx, y_known, x_missing_idx)\n",
        "        except:\n",
        "            pass\n",
        "        try:\n",
        "            candidates[\"spline\"] = Spline_prediction(x_known_idx, y_known, x_missing_idx)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        best_method = None\n",
        "        best_score = float(\"inf\")\n",
        "        for method, y_pred in candidates.items():\n",
        "            combined = np.concatenate([y_pred, y_known])\n",
        "            score = evaluate_smoothness(combined)\n",
        "            if score < best_score:\n",
        "                best_score = score\n",
        "                best_method = method\n",
        "\n",
        "        if best_method:\n",
        "            filled_data.iloc[i, 1:k_index+1] = candidates[best_method]\n",
        "            print(best_method)\n",
        "\n",
        "    return filled_data\n",
        "\n",
        "# ------------------- Apply Imputation -------------------\n",
        "\n",
        "filled_train = predict_missing_values(train)\n",
        "filled_train.to_csv(\"filled_train.csv\", index=False)\n",
        "print(\"Missing values in train filled and saved to 'filled_train.csv'.\")\n",
        "\n",
        "filled_test = predict_missing_values(test)\n",
        "filled_test.to_csv(\"filled_test.csv\", index=False)\n",
        "print(\"Missing values in test filled and saved to 'filled_test.csv'.\")\n",
        "\n",
        "# Remove geology_id and prepare clean input data\n",
        "X_train_clean = filled_train.iloc[:, 1:301]\n",
        "X_test_clean = filled_test.iloc[:, 1:301]\n",
        "\n",
        "# ------------------- LSTM Model Training -------------------\n",
        "\n",
        "# Normalize input features\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_clean)\n",
        "X_test_scaled = scaler.transform(X_test_clean)\n",
        "\n",
        "X_train_seq = X_train_scaled.reshape(-1, 300, 1)\n",
        "X_test_seq = X_test_scaled.reshape(-1, 300, 1)\n",
        "\n",
        "# Define LSTM model\n",
        "model = Sequential([\n",
        "    LSTM(128, return_sequences=False, input_shape=(300, 1)),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(300)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train model (no validation split)\n",
        "model.fit(X_train_seq, y_train_full.values, epochs=100, batch_size=16, verbose=1)\n",
        "\n",
        "# Predict on test set\n",
        "y_test_pred = model.predict(X_test_seq)\n",
        "\n",
        "# ------------------- Generate Submission File -------------------\n",
        "\n",
        "columns_new = [str(i+1) for i in range(300)]\n",
        "for r in range(1, 10):\n",
        "    for pos in range(1, 301):\n",
        "        columns_new.append(f\"r_{r}_pos_{pos}\")\n",
        "\n",
        "if y_test_pred.shape[1] < len(columns_new):\n",
        "    y_test_pred = np.pad(y_test_pred, ((0, 0), (0, len(columns_new) - y_test_pred.shape[1])), constant_values=np.nan)\n",
        "\n",
        "submission = pd.DataFrame(y_test_pred, columns=columns_new)\n",
        "submission.insert(0, \"geology_id\", test_ids)\n",
        "\n",
        "# Repeat the base prediction across all r_{r}_pos_{pos} fields\n",
        "copied_part = submission.iloc[:, 1:301].copy().values\n",
        "for start in range(301, 3001, 300):\n",
        "    submission.iloc[:, start:start+300] = copied_part\n",
        "\n",
        "submission.to_csv(\"voting_submission.csv\", index=False)\n",
        "print(\"âœ… voting_submission.csv created\")"
      ]
    }
  ]
}